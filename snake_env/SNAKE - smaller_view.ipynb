{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efe8abf-7236-4d0b-a3f8-25a961a0365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3095ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ceb64aa-53ab-49bd-b51c-7ff1efc19d97",
   "metadata": {},
   "source": [
    "# Maak de custom gymnasium omgeving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea08731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SnakeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(SnakeEnv, self).__init__()\n",
    "        self.grid_size = 20\n",
    "        self.cell_size = 30\n",
    "        self.action_space = spaces.Discrete(4)  # 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 7, 7), dtype=np.float32)\n",
    "        \n",
    "        self.view_radius = 3  # de afstand rond de head die de agent ziet\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.snake = [(5, 5), (5, 5), (5, 5)]\n",
    "        self.direction = 1\n",
    "        self.food = self._place_food()\n",
    "        self.done = False\n",
    "        if self.render_mode == \"human\":\n",
    "            self._init_render()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _place_food(self):\n",
    "        while True:\n",
    "            food = (random.randint(0, 19), random.randint(0, 19))\n",
    "            if food not in self.snake:\n",
    "                return food\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        view_size = 3  # 3 op elke kant van de kop ‚Üí 7x7\n",
    "        obs = np.zeros((3, 7, 7), dtype=np.float32)\n",
    "\n",
    "        head_x, head_y = self.snake[0]\n",
    "\n",
    "        for dy in range(-view_size, view_size + 1):\n",
    "            for dx in range(-view_size, view_size + 1):\n",
    "                x = head_x + dx\n",
    "                y = head_y + dy\n",
    "                view_x = dx + view_size\n",
    "                view_y = dy + view_size\n",
    "\n",
    "                if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
    "                    if (x, y) == self.snake[0]:\n",
    "                        obs[2, view_y, view_x] = 1  # Head\n",
    "                    elif (x, y) in self.snake[1:]:\n",
    "                        obs[0, view_y, view_x] = 1  # Body\n",
    "                    elif (x, y) == self.food:\n",
    "                        obs[1, view_y, view_x] = 1  # Food\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True, False, {}\n",
    "\n",
    "        if abs(action - self.direction) == 2:\n",
    "            action = self.direction\n",
    "\n",
    "        self.direction = action\n",
    "\n",
    "        dx = [0, 1, 0, -1]\n",
    "        dy = [-1, 0, 1, 0]\n",
    "        head_x, head_y = self.snake[0]\n",
    "        new_head = (head_x + dx[action], head_y + dy[action])\n",
    "\n",
    "        if (new_head in self.snake or\n",
    "            not 0 <= new_head[0] < self.grid_size or\n",
    "            not 0 <= new_head[1] < self.grid_size):\n",
    "            self.done = True\n",
    "            return self._get_obs(), -10, True, False, {}\n",
    "\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        fx, fy = self.food\n",
    "        old_dist = abs(head_x - fx) + abs(head_y - fy)\n",
    "        new_dist = abs(new_head[0] - fx) + abs(new_head[1] - fy)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.steps += 1\n",
    "\n",
    "        if new_head == self.food:\n",
    "            reward = 50\n",
    "            self.food = self._place_food()\n",
    "            self.steps = 0  \n",
    "        else:\n",
    "            reward = (old_dist - new_dist) * 0.5 - 0.2\n",
    "            self.snake.pop()\n",
    "\n",
    "        if self.steps >= 100:\n",
    "            reward= -10\n",
    "            self.done = True\n",
    "            return self._get_obs(), reward, True, False, {}\n",
    "\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "\n",
    "        return self._get_obs(), reward, False, False, {}\n",
    "\n",
    "    def _init_render(self):\n",
    "        pygame.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "            (self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        pygame.display.set_caption(\"Snake AI\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def render(self):\n",
    "        if self.window is None:\n",
    "            self._init_render()\n",
    "\n",
    "        self.window.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.window,\n",
    "                (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        fx, fy = self.food\n",
    "        pygame.draw.rect(\n",
    "            self.window,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "        )\n",
    "                        # === Highlight agent's view radius ===\n",
    "        if self.snake:\n",
    "            head_x, head_y = self.snake[0]\n",
    "            r = self.view_radius\n",
    "            for dy in range(-r, r+1):\n",
    "                for dx in range(-r, r+1):\n",
    "                    nx, ny = head_x + dx, head_y + dy\n",
    "                    if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:\n",
    "                        pygame.draw.rect(\n",
    "                            self.window,\n",
    "                            (50, 50, 50),  # blauw = zichtgebied\n",
    "                            pygame.Rect(nx * self.cell_size, ny * self.cell_size, self.cell_size, self.cell_size),\n",
    "                            width=1  # alleen rand tekenen\n",
    "                        )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(10)\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c27efa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_snake_tensorboard/PPO_23\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m callback \u001b[38;5;241m=\u001b[39m EvalAndRenderCallback(eval_env\u001b[38;5;241m=\u001b[39meval_env, render_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#model.learn(total_timesteps=100_000, callback=None)\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_snake_smallerview_5M_2.5learningrate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Om later opnieuw te laden:\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# model = PPO.load(\"ppo_snake\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     61\u001b[0m         )\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EvalAndRenderCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, render_freq=10_000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.render_freq = render_freq\n",
    "        self.episodes_run = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.render_freq == 0:\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, truncated, info = self.eval_env.step(action)\n",
    "                self.eval_env.render()\n",
    "        return True\n",
    "\n",
    "\n",
    "# Maak een instance van je custom omgeving\n",
    "env = SnakeEnv(render_mode=None)\n",
    "\n",
    "# Check of alles compatibel is\n",
    "#check_env(env)\n",
    "\n",
    "# Wrap de omgeving (nodig voor Stable-Baselines3)\n",
    "vec_env = DummyVecEnv([lambda: SnakeEnv(render_mode=None)])\n",
    "\n",
    "# Initialiseer PPO\n",
    "#model = PPO(\"MlpPolicy\", vec_env, verbose=1, learning_rate=1e-4, n_steps=2048, batch_size=64, n_epochs=10,\n",
    "#    tensorboard_log=\"./ppo_snake_tensorboard/\",device='cpu')\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",             # of \"CnnPolicy\" bij 3D observaties\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    tensorboard_log=\"./ppo_snake_tensorboard/\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "eval_env = SnakeEnv(render_mode=\"human\")\n",
    "callback = EvalAndRenderCallback(eval_env=eval_env, render_freq=1000)\n",
    "\n",
    "#model.learn(total_timesteps=100_000, callback=None)\n",
    "model.learn(total_timesteps=100_000, callback=None)\n",
    "\n",
    "\n",
    "model.save(\"ppo_snake_smallerview_5M_2.5learningrate\")\n",
    "# Om later opnieuw te laden:\n",
    "# model = PPO.load(\"ppo_snake\")\n",
    "\n",
    "test_env = SnakeEnv(render_mode=\"human\")\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = test_env.step(action)\n",
    "    test_env.render()\n",
    "\n",
    "test_env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ec13f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "‚úÖ Episode 1 finished.\n",
      "üî∏ Reward: 83.10\n",
      "üçé Food eaten: 2\n",
      "üèÜ Highscore so far: 2\n",
      "\n",
      "‚úÖ Episode 2 finished.\n",
      "üî∏ Reward: -19.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 2\n",
      "\n",
      "‚úÖ Episode 3 finished.\n",
      "üî∏ Reward: 41.00\n",
      "üçé Food eaten: 1\n",
      "üèÜ Highscore so far: 2\n",
      "\n",
      "‚úÖ Episode 4 finished.\n",
      "üî∏ Reward: 38.70\n",
      "üçé Food eaten: 1\n",
      "üèÜ Highscore so far: 2\n",
      "\n",
      "‚úÖ Episode 5 finished.\n",
      "üî∏ Reward: -21.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 2\n",
      "\n",
      "‚úÖ Episode 6 finished.\n",
      "üî∏ Reward: 139.50\n",
      "üçé Food eaten: 3\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 7 finished.\n",
      "üî∏ Reward: -14.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 8 finished.\n",
      "üî∏ Reward: -13.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 9 finished.\n",
      "üî∏ Reward: -10.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 10 finished.\n",
      "üî∏ Reward: -16.60\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "==== Test Summary ====\n",
      "Average reward: 20.57\n",
      "Average food per episode: 0.70\n",
      "Highscore (most food): 3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Recreate the environment (no training wrapper needed)\n",
    "eval_env = SnakeEnv(render_mode=\"human\")  # enable rendering\n",
    "\n",
    "# Load your trained model\n",
    "model = PPO.load(\"ppo_snake_smallerview_2M_2.5learningrate\", env=eval_env,device='cpu')\n",
    "\n",
    "num_episodes = 10\n",
    "total_rewards = []\n",
    "food_counts = []\n",
    "\n",
    "highscore = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "    food_eaten = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        prev_snake_len = len(eval_env.snake)\n",
    "\n",
    "        obs, reward, done, truncated, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Check of de slang is gegroeid (dus voedsel gegeten)\n",
    "        if len(eval_env.snake) > prev_snake_len:\n",
    "            food_eaten += 1\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    food_counts.append(food_eaten)\n",
    "    highscore = max(highscore, food_eaten)\n",
    "\n",
    "    print(f\"\\n‚úÖ Episode {episode+1} finished.\")\n",
    "    print(f\"üî∏ Reward: {episode_reward:.2f}\")\n",
    "    print(f\"üçé Food eaten: {food_eaten}\")\n",
    "    print(f\"üèÜ Highscore so far: {highscore}\")\n",
    "\n",
    "# Na alle episodes\n",
    "avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "avg_food = sum(food_counts) / len(food_counts)\n",
    "\n",
    "print(\"\\n==== Test Summary ====\")\n",
    "print(f\"Average reward: {avg_reward:.2f}\")\n",
    "print(f\"Average food per episode: {avg_food:.2f}\")\n",
    "print(f\"Highscore (most food): {highscore}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "204bac0b-d6af-4f7b-9e3b-68e635bf0f86",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = {}\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_state(self, obs):\n",
    "        return tuple(obs.flatten())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon or state not in self.q_table:\n",
    "            return random.choice(range(self.actions))\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.actions)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(self.actions)\n",
    "\n",
    "        predict = self.q_table[state][action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31aeca0e-fdd4-4f84-9718-70e3944448eb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "env = SnakeEnv(render_mode=\"human\")\n",
    "agent = QLearningAgent(actions=4)\n",
    "\n",
    "for episode in range(100000):\n",
    "    if episode % 1000 == 0:\n",
    "        env.render_mode = \"human\"\n",
    "    else:\n",
    "        env.render_mode = None\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    state = agent.get_state(obs)\n",
    "    total_reward = 0\n",
    "\n",
    "    #for step in range(1000):\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, terminated, _, _ = env.step(action)\n",
    "        next_state = agent.get_state(next_obs)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351537b6-31dd-4055-816a-c5176ff78e65",
   "metadata": {},
   "source": [
    "# Registreer de omgeving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7c0f1a3-ab68-4527-859f-e2a2aa8d87fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 16380), started 0:00:50 ago. (Use '!kill 16380' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7754a6c2559d1f97\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7754a6c2559d1f97\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir='ppo_snake_tensorboard'\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ccbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e9cd0e50",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\"\"\"    def _get_obs(self):\n",
    "        view_size = 3  # 3 op elke kant van de kop ‚Üí 7x7\n",
    "        obs = np.zeros((3, 7, 7), dtype=np.float32)\n",
    "\n",
    "        head_x, head_y = self.snake[0]\n",
    "\n",
    "        for dy in range(-view_size, view_size + 1):\n",
    "            for dx in range(-view_size, view_size + 1):\n",
    "                x = head_x + dx\n",
    "                y = head_y + dy\n",
    "                view_x = dx + view_size\n",
    "                view_y = dy + view_size\n",
    "\n",
    "                if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
    "                    if (x, y) == self.snake[0]:\n",
    "                        obs[2, view_y, view_x] = 1  # Head\n",
    "                    elif (x, y) in self.snake[1:]:\n",
    "                        obs[0, view_y, view_x] = 1  # Body\n",
    "                    elif (x, y) == self.food:\n",
    "                        obs[1, view_y, view_x] = 1  # Food\n",
    "\n",
    "        return obs\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fb4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
