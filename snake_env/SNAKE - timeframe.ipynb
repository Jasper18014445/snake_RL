{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe8abf-7236-4d0b-a3f8-25a961a0365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "SNAKE.ipynb\n",
    "SNAKE - head.ipynb\n",
    "SNAKE - smaller_view.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb64aa-53ab-49bd-b51c-7ff1efc19d97",
   "metadata": {},
   "source": [
    "# Maak de custom gymnasium omgeving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdfd78-4a93-49fa-87f8-91a3cd94b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SnakeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(SnakeEnv, self).__init__()\n",
    "        self.grid_size = 20\n",
    "        self.cell_size = 30\n",
    "        self.action_space = spaces.Discrete(4)  # 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=2,\n",
    "            shape=(3, self.grid_size, self.grid_size),\n",
    "            dtype=np.int32\n",
    ")\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.obs_buffer = []    \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.snake = [(5, 5), (5, 5), (5, 5)]\n",
    "        self.direction = 1\n",
    "        self.food = self._place_food()\n",
    "        self.done = False\n",
    "        if self.render_mode == \"human\":\n",
    "            self._init_render()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _place_food(self):\n",
    "        while True:\n",
    "            food = (random.randint(0, 9), random.randint(0, 9))\n",
    "            if food not in self.snake:\n",
    "                return food\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros((self.grid_size, self.grid_size), dtype=np.int32)\n",
    "        for x, y in self.snake:\n",
    "            obs[y][x] = 1\n",
    "        \n",
    "        head_x, head_y = self.snake[0]\n",
    "        obs[head_y][head_x] = 3  # üëà Mark head as 3\n",
    "        \n",
    "        fx, fy = self.food\n",
    "        obs[fy][fx] = 2\n",
    "    \n",
    "        self.obs_buffer.append(obs)\n",
    "        if len(self.obs_buffer) > 3:\n",
    "            self.obs_buffer.pop(0)\n",
    "    \n",
    "        while len(self.obs_buffer) < 3:\n",
    "            self.obs_buffer.insert(0, np.zeros((self.grid_size, self.grid_size), dtype=np.int32))\n",
    "    \n",
    "        return np.array(self.obs_buffer)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True, False, {}\n",
    "\n",
    "        if abs(action - self.direction) == 2:\n",
    "            action = self.direction\n",
    "\n",
    "        self.direction = action\n",
    "\n",
    "        dx = [0, 1, 0, -1]\n",
    "        dy = [-1, 0, 1, 0]\n",
    "        head_x, head_y = self.snake[0]\n",
    "        new_head = (head_x + dx[action], head_y + dy[action])\n",
    "\n",
    "        if (new_head in self.snake or\n",
    "            not 0 <= new_head[0] < self.grid_size or\n",
    "            not 0 <= new_head[1] < self.grid_size):\n",
    "            self.done = True\n",
    "            return self._get_obs(), -10, True, False, {}\n",
    "\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        fx, fy = self.food\n",
    "        old_dist = abs(head_x - fx) + abs(head_y - fy)\n",
    "        new_dist = abs(new_head[0] - fx) + abs(new_head[1] - fy)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.steps += 1\n",
    "\n",
    "        if new_head == self.food:\n",
    "            reward = 50\n",
    "            self.food = self._place_food()\n",
    "            self.steps = 0  \n",
    "        else:\n",
    "            reward = (old_dist - new_dist) * 0.5 - 0.2\n",
    "            self.snake.pop()\n",
    "\n",
    "        if self.steps >= 100:\n",
    "            reward= -10\n",
    "            self.done = True\n",
    "            return self._get_obs(), reward, True, False, {}\n",
    "\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "\n",
    "        return self._get_obs(), reward, False, False, {}\n",
    "\n",
    "    def _init_render(self):\n",
    "        pygame.init()\n",
    "        self.window = pygame.display.set_mode(\n",
    "            (self.grid_size * self.cell_size, self.grid_size * self.cell_size))\n",
    "        pygame.display.set_caption(\"Snake AI\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def render(self):\n",
    "        if self.window is None:\n",
    "            self._init_render()\n",
    "\n",
    "        self.window.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.window,\n",
    "                (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        fx, fy = self.food\n",
    "        pygame.draw.rect(\n",
    "            self.window,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "        )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(10)\n",
    "\n",
    "    def close(self):\n",
    "        if self.window:\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27efa85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SnakeEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Maak een instance van je custom omgeving\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mSnakeEnv\u001b[49m(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check of alles compatibel is\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#check_env(env)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Wrap de omgeving (nodig voor Stable-Baselines3)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: SnakeEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SnakeEnv' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EvalAndRenderCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, render_freq=10_000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.render_freq = render_freq\n",
    "        self.episodes_run = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.num_timesteps % self.render_freq == 0:\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, truncated, info = self.eval_env.step(action)\n",
    "                self.eval_env.render()\n",
    "        return True\n",
    "\n",
    "\n",
    "# Maak een instance van je custom omgeving\n",
    "env = SnakeEnv(render_mode=None)\n",
    "\n",
    "# Check of alles compatibel is\n",
    "#check_env(env)\n",
    "\n",
    "# Wrap de omgeving (nodig voor Stable-Baselines3)\n",
    "vec_env = DummyVecEnv([lambda: SnakeEnv(render_mode=None)])\n",
    "\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",             \n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    tensorboard_log=\"./ppo_snake_tensorboard/\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "eval_env = SnakeEnv(render_mode=\"human\")\n",
    "callback = EvalAndRenderCallback(eval_env=eval_env, render_freq=1000)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=100_000, callback=None)\n",
    "\n",
    "model.save(\"ppo_snake_timeframe.5lr\")\n",
    "# Om later opnieuw te laden:\n",
    "# model = PPO.load(\"ppo_snake\")\n",
    "\n",
    "test_env = SnakeEnv(render_mode=\"human\")\n",
    "obs, _ = test_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = test_env.step(action)\n",
    "    test_env.render()\n",
    "\n",
    "test_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f90924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Recreate the environment (no training wrapper needed)\n",
    "eval_env = SnakeEnv(render_mode=\"human\")  # enable rendering\n",
    "\n",
    "# Load your trained model\n",
    "model = PPO.load(\"ppo_snake_timeframe_10M\", env=eval_env,device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40fbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jaspe\\.conda\\envs\\Pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Episode 1 finished.\n",
      "üî∏ Reward: 41.20\n",
      "üçé Food eaten: 1\n",
      "üèÜ Highscore so far: 1\n",
      "\n",
      "‚úÖ Episode 2 finished.\n",
      "üî∏ Reward: 139.80\n",
      "üçé Food eaten: 3\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 3 finished.\n",
      "üî∏ Reward: 90.10\n",
      "üçé Food eaten: 2\n",
      "üèÜ Highscore so far: 3\n",
      "\n",
      "‚úÖ Episode 4 finished.\n",
      "üî∏ Reward: 292.10\n",
      "üçé Food eaten: 6\n",
      "üèÜ Highscore so far: 6\n",
      "\n",
      "‚úÖ Episode 5 finished.\n",
      "üî∏ Reward: 138.50\n",
      "üçé Food eaten: 3\n",
      "üèÜ Highscore so far: 6\n",
      "\n",
      "‚úÖ Episode 6 finished.\n",
      "üî∏ Reward: 86.80\n",
      "üçé Food eaten: 2\n",
      "üèÜ Highscore so far: 6\n",
      "\n",
      "‚úÖ Episode 7 finished.\n",
      "üî∏ Reward: 188.10\n",
      "üçé Food eaten: 4\n",
      "üèÜ Highscore so far: 6\n",
      "\n",
      "‚úÖ Episode 8 finished.\n",
      "üî∏ Reward: -8.20\n",
      "üçé Food eaten: 0\n",
      "üèÜ Highscore so far: 6\n",
      "\n",
      "‚úÖ Episode 9 finished.\n",
      "üî∏ Reward: 339.00\n",
      "üçé Food eaten: 7\n",
      "üèÜ Highscore so far: 7\n",
      "\n",
      "‚úÖ Episode 10 finished.\n",
      "üî∏ Reward: 136.60\n",
      "üçé Food eaten: 3\n",
      "üèÜ Highscore so far: 7\n",
      "\n",
      "==== Test Summary ====\n",
      "Average reward: 144.40\n",
      "Average food per episode: 3.10\n",
      "Highscore (most food): 7\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Recreate the environment (no training wrapper needed)\n",
    "eval_env = SnakeEnv(render_mode=\"human\")  # enable rendering\n",
    "\n",
    "# Load your trained model\n",
    "model_path = \"ppo_snake_timeframe_10M.zip\"\n",
    "model = PPO.load(model_path, env=eval_env,device='cpu')\n",
    "\n",
    "num_episodes = 10\n",
    "total_rewards = []\n",
    "food_counts = []\n",
    "\n",
    "highscore = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "    food_eaten = 0\n",
    "    steps_since_food = 0  \n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        prev_snake_len = len(eval_env.snake)\n",
    "\n",
    "        obs, reward, done, truncated, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "        steps_since_food += 1  # tel stappen sinds laatste appel\n",
    "\n",
    "        # Check of voedsel is gegeten\n",
    "        if len(eval_env.snake) > prev_snake_len:\n",
    "            food_eaten += 1\n",
    "            steps_since_food = 0  # reset bij eten\n",
    "\n",
    "        # Stop als er 200 stappen geen voedsel is gegeten\n",
    "        if steps_since_food >= 200:\n",
    "            print(\"‚ö†Ô∏è  Terminating: no food eaten in 200 steps.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    food_counts.append(food_eaten)\n",
    "    highscore = max(highscore, food_eaten)\n",
    "\n",
    "    print(f\"\\n‚úÖ Episode {episode+1} finished.\")\n",
    "    print(f\"üî∏ Reward: {episode_reward:.2f}\")\n",
    "    print(f\"üçé Food eaten: {food_eaten}\")\n",
    "    print(f\"üèÜ Highscore so far: {highscore}\")\n",
    "\n",
    "# Na alle episodes\n",
    "avg_reward = sum(total_rewards) / len(total_rewards)\n",
    "avg_food = sum(food_counts) / len(food_counts)\n",
    "\n",
    "print(\"\\n==== Test Summary ====\")\n",
    "print(f\"Average reward: {avg_reward:.2f}\")\n",
    "print(f\"Average food per episode: {avg_food:.2f}\")\n",
    "print(f\"Highscore (most food): {highscore}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "204bac0b-d6af-4f7b-9e3b-68e635bf0f86",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = {}\n",
    "        self.actions = actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_state(self, obs):\n",
    "        return tuple(obs.flatten())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon or state not in self.q_table:\n",
    "            return random.choice(range(self.actions))\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.actions)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(self.actions)\n",
    "\n",
    "        predict = self.q_table[state][action]\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31aeca0e-fdd4-4f84-9718-70e3944448eb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "env = SnakeEnv(render_mode=\"human\")\n",
    "agent = QLearningAgent(actions=4)\n",
    "\n",
    "for episode in range(100000):\n",
    "    if episode % 1000 == 0:\n",
    "        env.render_mode = \"human\"\n",
    "    else:\n",
    "        env.render_mode = None\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    state = agent.get_state(obs)\n",
    "    total_reward = 0\n",
    "\n",
    "    #for step in range(1000):\n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_obs, reward, terminated, _, _ = env.step(action)\n",
    "        next_state = agent.get_state(next_obs)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode+1}: Total reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fb4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
